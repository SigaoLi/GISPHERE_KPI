"""
GISource å›¢é˜Ÿç»©æ•ˆç®¡ç†é¢æ¿
Performance Monitoring Dashboard for GISource Team

è¿è¡Œæ–¹å¼: streamlit run Dashboard.py
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta, date
import pytz
import pickle
import os
import mysql.connector
import configparser
import warnings
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import re

# é…ç½®
warnings.filterwarnings('ignore')
china_tz = pytz.timezone('Asia/Shanghai')
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
SPREADSHEET_ID = '1LcfxcTCuj9ZJXXMxyFQwt-xnbAviNP8j9oDr6OG5-Go'


# ==================== æ•°æ®è·å–å‡½æ•° ====================

@st.cache_resource
def authorize_credentials():
    """è°·æ­Œ API å¯†é’¥å‡­æ®"""
    creds = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)
    return creds


def fetch_google_sheet_data(range_name):
    """ä» Google Sheet è·å–æ•°æ®"""
    try:
        creds = authorize_credentials()
        service = build('sheets', 'v4', credentials=creds)
        sheet = service.spreadsheets()
        result = sheet.values().get(spreadsheetId=SPREADSHEET_ID, range=range_name).execute()
        values = result.get('values', [])
        
        if not values:
            return pd.DataFrame()
        
        headers = values[0]
        data = values[1:]
        
        # è°ƒæ•´åˆ—æ•°
        adjusted_data = []
        for row in data:
            adjusted_row = row + [None] * (len(headers) - len(row))
            adjusted_data.append(adjusted_row)
        
        return pd.DataFrame(adjusted_data, columns=headers)
    except Exception as e:
        st.error(f"è¯»å– Google Sheet å‡ºé”™: {str(e)}")
        return pd.DataFrame()


@st.cache_resource
def connect_to_database():
    """è¿æ¥åˆ° MySQL æ•°æ®åº“"""
    try:
        config = configparser.ConfigParser()
        config.read('sql_credentials.txt')
        
        mysql_config = {
            'host': config['MySQL']['host'],
            'port': config['MySQL'].getint('port', 3306),
            'user': config['MySQL']['user'],
            'password': config['MySQL']['password'],
            'database': config['MySQL']['database']
        }
        
        conn = mysql.connector.connect(**mysql_config)
        return conn
    except Exception as e:
        st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        return None


def fetch_mysql_data():
    """ä» MySQL è·å–æ•°æ®"""
    try:
        conn = connect_to_database()
        if not conn:
            return pd.DataFrame()
        
        query = """
        SELECT 
            Event_ID,
            University_CN,
            University_EN,
            Country_CN,
            Job_CN,
            Job_EN,
            Description,
            Title_CN,
            Title_EN,
            Date,
            IS_Public,
            IS_Deleted
        FROM GISource
        WHERE IS_Deleted = 0
        ORDER BY Date DESC
        """
        
        df = pd.read_sql(query, conn)
        conn.close()
        return df
    except Exception as e:
        st.error(f"è¯»å–æ•°æ®åº“å‡ºé”™: {str(e)}")
        return pd.DataFrame()


# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================

def parse_db_description(df):
    """è§£æ MySQL Description å­—æ®µï¼Œæå– URL å’Œ Deadline"""
    if df.empty:
        return df
    
    # æå– URL
    url_pattern = r"URL:\s*(https?://[^\s<]+)"
    df['Extracted_Source'] = df['Description'].str.extract(url_pattern, flags=re.IGNORECASE)
    
    # æå– Deadline - æ”¯æŒæ—¥æœŸæ ¼å¼å’Œ "Soon"
    date_pattern = r"Deadline:\s*(\d{4}-\d{2}-\d{2}|Soon)"
    df['Extracted_Deadline'] = df['Description'].str.extract(date_pattern, flags=re.IGNORECASE)
    
    # åˆ›å»ºå¤åˆé”® (Composite Key) = URL + Deadline
    df['Composite_Key'] = (
        df['Extracted_Source'].fillna('').str.strip() + "_" + 
        df['Extracted_Deadline'].fillna('').str.strip()
    )
    
    return df


def prepare_sheet_data(df):
    """å‡†å¤‡ Google Sheet æ•°æ®ï¼Œç”Ÿæˆå¤åˆé”®"""
    if df.empty:
        return df
    
    # å¤„ç† Deadlineï¼šæ”¯æŒæ—¥æœŸæ ¼å¼ã€Excelåºåˆ—å·å’Œ "Soon"
    def format_deadline(val):
        if pd.isna(val) or val == '':
            return ''
        if str(val).strip().lower() == 'soon':
            return 'Soon'
        
        # å°è¯•æ£€æµ‹ Excel åºåˆ—æ—¥æœŸï¼ˆæ•°å­—æ ¼å¼ï¼Œé€šå¸¸åœ¨ 1-100000 èŒƒå›´å†…ï¼‰
        try:
            # å…ˆå°è¯•è½¬æ¢ä¸ºæ•°å­—
            num_val = float(str(val).strip())
            # å¦‚æœæ˜¯åˆç†çš„ Excel æ—¥æœŸåºåˆ—å·ï¼ˆ1900-01-01 åˆ°æœªæ¥å‡ åå¹´ï¼‰
            if 1 <= num_val <= 100000:
                # Excel æ—¥æœŸä» 1900-01-01 å¼€å§‹è®¡æ•°
                # pandas çš„ to_datetime å¯ä»¥å¤„ç† Excel åºåˆ—å·
                excel_date = pd.Timestamp('1899-12-30') + pd.Timedelta(days=num_val)
                return excel_date.strftime('%Y-%m-%d')
        except (ValueError, TypeError):
            pass
        
        # å°è¯•æ ‡å‡†æ—¥æœŸè§£æ
        try:
            return pd.to_datetime(val).strftime('%Y-%m-%d')
        except:
            return ''
    
    df['Deadline_Str'] = df['Deadline'].apply(format_deadline)
    
    # åˆ›å»ºå¤åˆé”®
    df['Composite_Key'] = (
        df['Source'].fillna('').str.strip() + "_" + 
        df['Deadline_Str'].str.strip()
    )
    
    return df


def merge_data():
    """åˆå¹¶ Google Sheet å’Œ MySQL æ•°æ®"""
    # è·å–æ•°æ®
    with st.spinner('æ­£åœ¨ä» Google Sheet è¯»å–æ•°æ®...'):
        filled_data = fetch_google_sheet_data('Filled')
    
    with st.spinner('æ­£åœ¨ä»æ•°æ®åº“è¯»å–æ•°æ®...'):
        db_data = fetch_mysql_data()
    
    if filled_data.empty or db_data.empty:
        st.warning("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥è¡¨")
        return pd.DataFrame()
    
    # å¤„ç†æ•°æ®
    db_data = parse_db_description(db_data)
    filled_data = prepare_sheet_data(filled_data)
    
    # åˆå¹¶æ•°æ®
    merged_df = pd.merge(
        db_data,
        filled_data[['Composite_Key', 'Verifier', 'Direction', 'University_CN']],
        on='Composite_Key',
        how='inner',
        suffixes=('_DB', '_Sheet')
    )
    
    # æ•°æ®æ¸…ç†
    if not merged_df.empty:
        # è½¬æ¢æ—¥æœŸ
        merged_df['Date'] = pd.to_datetime(merged_df['Date'])
        
        # å¤„ç† Deadlineï¼šå°† "Soon" æŒ‰ç…§ 30 å¤©è®¡ç®—ï¼Œæ”¯æŒ Excel åºåˆ—å·
        def parse_deadline(val, entry_date):
            if pd.isna(val) or val == '':
                return pd.NaT
            if str(val).strip().lower() == 'soon':
                # Soon æŒ‰ç…§å…¥åº“æ—¥æœŸ + 30 å¤©è®¡ç®—
                return entry_date + timedelta(days=30)
            
            # å°è¯•æ£€æµ‹ Excel åºåˆ—æ—¥æœŸ
            try:
                num_val = float(str(val).strip())
                if 1 <= num_val <= 100000:
                    # Excel æ—¥æœŸä» 1899-12-30 å¼€å§‹è®¡æ•°
                    excel_date = pd.Timestamp('1899-12-30') + pd.Timedelta(days=num_val)
                    return excel_date
            except (ValueError, TypeError):
                pass
            
            # æ ‡å‡†æ—¥æœŸè§£æ
            try:
                return pd.to_datetime(val)
            except:
                return pd.NaT
        
        merged_df['Extracted_Deadline_Date'] = merged_df.apply(
            lambda row: parse_deadline(row['Extracted_Deadline'], row['Date']), 
            axis=1
        )
        
        # è¿‡æ»¤æ‰ Verifier ä¸ºç©ºæˆ–ä¸º LLM çš„æ•°æ®
        merged_df = merged_df[
            (merged_df['Verifier'].notna()) & 
            (merged_df['Verifier'] != 'LLM') &
            (merged_df['Verifier'] != '')
        ]
    
    return merged_df


# ==================== å¯è§†åŒ–ç»„ä»¶ ====================

def display_kpi_metrics(filtered_data):
    """æ˜¾ç¤ºå…³é”®æŒ‡æ ‡"""
    col1, col2, col3, col4 = st.columns(4)
    
    total_entries = len(filtered_data)
    active_members = filtered_data['Verifier'].nunique() if not filtered_data.empty else 0
    
    # è®¡ç®—å¹³å‡æå‰å¤©æ•°ï¼ˆéœ€è¦å…ˆè®¡ç®— Lead_Timeï¼‰
    avg_lead_time = 0
    if not filtered_data.empty and 'Extracted_Deadline_Date' in filtered_data.columns:
        # è®¡ç®— Lead_Time
        filtered_data_copy = filtered_data.copy()
        filtered_data_copy['Lead_Time'] = (
            filtered_data_copy['Extracted_Deadline_Date'] - filtered_data_copy['Date']
        ).dt.days
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_lead_times = filtered_data_copy[
            (filtered_data_copy['Lead_Time'].notna()) & 
            (filtered_data_copy['Lead_Time'] >= 0)
        ]
        avg_lead_time = valid_lead_times['Lead_Time'].mean() if not valid_lead_times.empty else 0
    
    col1.metric("ğŸ“Š å…¥åº“æ€»æ•°", total_entries)
    col2.metric("ğŸ‘¥ æ´»è·ƒæˆå‘˜æ•°", active_members)
    col3.metric("â° å¹³å‡æå‰å¤©æ•°", f"{avg_lead_time:.1f} å¤©")
    
    # æœ€è¿‘7å¤©æ–°å¢
    if not filtered_data.empty:
        today = datetime.now(china_tz).date()
        seven_days_ago = today - timedelta(days=7)
        recent_data = filtered_data[filtered_data['Date'].dt.date >= seven_days_ago]
        col4.metric("ğŸ“ˆ æœ€è¿‘7å¤©æ–°å¢", len(recent_data))


def display_member_leaderboard(filtered_data):
    """æˆå‘˜è´¡çŒ®æ’è¡Œæ¦œ"""
    st.subheader("ğŸ† æˆå‘˜è´¡çŒ®æ’è¡Œ")
    
    if filtered_data.empty:
        st.info("æš‚æ— æ•°æ®")
        return
    
    # ç»Ÿè®¡æ¯ä¸ªäººçš„è´¡çŒ®
    leaderboard = filtered_data['Verifier'].value_counts().reset_index()
    leaderboard.columns = ['æˆå‘˜', 'å…¥åº“æ•°é‡']
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    fig = px.bar(
        leaderboard,
        x='æˆå‘˜',
        y='å…¥åº“æ•°é‡',
        text='å…¥åº“æ•°é‡',
        color='å…¥åº“æ•°é‡',
        color_continuous_scale='Blues',
        title='æˆå‘˜å…¥åº“æ•°é‡æ’è¡Œ'
    )
    fig.update_traces(textposition='outside')
    fig.update_layout(showlegend=False, height=400)
    st.plotly_chart(fig, width='stretch')
    
    # æ˜¾ç¤ºè¯¦ç»†è¡¨æ ¼
    with st.expander("æŸ¥çœ‹è¯¦ç»†æ•°æ®"):
        st.dataframe(leaderboard, width='stretch')


def display_daily_trend(filtered_data):
    """æ¯æ—¥å·¥ä½œè¶‹åŠ¿"""
    st.subheader("ğŸ“ˆ æ¯æ—¥å·¥ä½œè¶‹åŠ¿")
    
    if filtered_data.empty:
        st.info("æš‚æ— æ•°æ®")
        return
    
    # æŒ‰æ—¥æœŸå’Œæˆå‘˜åˆ†ç»„ç»Ÿè®¡
    trend = filtered_data.groupby([filtered_data['Date'].dt.date, 'Verifier']).size().reset_index(name='Count')
    trend.columns = ['Date', 'Verifier', 'Count']
    
    # ç»˜åˆ¶æŠ˜çº¿å›¾
    fig = px.line(
        trend,
        x='Date',
        y='Count',
        color='Verifier',
        markers=True,
        title='æˆå‘˜æ¯æ—¥è´¡çŒ®è¶‹åŠ¿'
    )
    fig.update_layout(
        xaxis_title='æ—¥æœŸ',
        yaxis_title='å…¥åº“æ•°é‡',
        height=400,
        hovermode='x unified'
    )
    st.plotly_chart(fig, width='stretch')


def display_lead_time_analysis(filtered_data):
    """ä¿¡æ¯æ—¶æ•ˆæ€§åˆ†æ"""
    st.subheader("â³ ä¿¡æ¯æ—¶æ•ˆæ€§åˆ†æ")
    
    if filtered_data.empty:
        st.info("æš‚æ— æ•°æ®")
        return
    
    # è®¡ç®—æå‰æœŸï¼šDeadline - å…¥åº“æ—¶é—´ï¼ˆSoon æŒ‰ 30 å¤©è®¡ç®—ï¼‰
    if 'Extracted_Deadline_Date' in filtered_data.columns:
        filtered_data['Lead_Time'] = (
            filtered_data['Extracted_Deadline_Date'] - filtered_data['Date']
        ).dt.days
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®ï¼ˆæå‰æœŸ >= 0ï¼‰
        valid_data = filtered_data[
            (filtered_data['Lead_Time'].notna()) & 
            (filtered_data['Lead_Time'] >= 0)
        ]
        
        if valid_data.empty:
            st.info("æš‚æ— æœ‰æ•ˆæ—¶æ•ˆæ€§æ•°æ®")
            return
        
        # æŒ‰æˆå‘˜ç»Ÿè®¡å¹³å‡æå‰æœŸ
        avg_lead = valid_data.groupby('Verifier')['Lead_Time'].mean().reset_index()
        avg_lead = avg_lead.sort_values('Lead_Time', ascending=False)
        
        # ç»˜åˆ¶æ¡å½¢å›¾
        fig = px.bar(
            avg_lead,
            x='Verifier',
            y='Lead_Time',
            text='Lead_Time',
            color='Lead_Time',
            color_continuous_scale='RdYlGn',
            title='æˆå‘˜å¹³å‡æå‰å‘å¸ƒå¤©æ•°'
        )
        fig.update_traces(texttemplate='%{text:.1f}', textposition='outside')
        fig.update_layout(
            xaxis_title='æˆå‘˜',
            yaxis_title='å¹³å‡æå‰å¤©æ•°',
            showlegend=False,
            height=400
        )
        st.plotly_chart(fig, width='stretch')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2 = st.columns(2)
        with col1:
            st.metric("å¹³å‡æå‰å¤©æ•°", f"{valid_data['Lead_Time'].mean():.1f} å¤©")
        with col2:
            st.metric("æœ€å¤§æå‰å¤©æ•°", f"{valid_data['Lead_Time'].max():.0f} å¤©")


def display_country_distribution(filtered_data):
    """å›½å®¶åˆ†å¸ƒåˆ†æ"""
    st.subheader("ğŸŒ å›½å®¶åˆ†å¸ƒ")
    
    if filtered_data.empty:
        st.info("æš‚æ— æ•°æ®")
        return
    
    # ç»Ÿè®¡å›½å®¶åˆ†å¸ƒ
    country_dist = filtered_data['Country_CN'].value_counts().reset_index()
    country_dist.columns = ['å›½å®¶', 'æ•°é‡']
    country_dist = country_dist.head(10)  # åªæ˜¾ç¤ºå‰10
    
    # ç»˜åˆ¶é¥¼å›¾
    fig = px.pie(
        country_dist,
        values='æ•°é‡',
        names='å›½å®¶',
        title='Top 10 å›½å®¶åˆ†å¸ƒ'
    )
    fig.update_traces(textposition='inside', textinfo='percent+label')
    st.plotly_chart(fig, width='stretch')


def display_job_type_distribution(filtered_data):
    """èŒä½ç±»å‹åˆ†æ"""
    st.subheader("ğŸ’¼ èŒä½ç±»å‹åˆ†å¸ƒ")
    
    if filtered_data.empty:
        st.info("æš‚æ— æ•°æ®")
        return
    
    # ç»Ÿè®¡èŒä½ç±»å‹
    job_dist = filtered_data['Job_CN'].value_counts().reset_index()
    job_dist.columns = ['èŒä½ç±»å‹', 'æ•°é‡']
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    fig = px.bar(
        job_dist,
        x='èŒä½ç±»å‹',
        y='æ•°é‡',
        text='æ•°é‡',
        color='æ•°é‡',
        color_continuous_scale='Viridis'
    )
    fig.update_traces(textposition='outside')
    fig.update_layout(showlegend=False, height=400)
    st.plotly_chart(fig, width='stretch')


def display_data_table(filtered_data):
    """æ˜¾ç¤ºåŸå§‹æ•°æ®è¡¨"""
    st.subheader("ğŸ“‹ åŸå§‹æ•°æ®")
    
    if filtered_data.empty:
        st.info("æš‚æ— æ•°æ®")
        return
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
    display_columns = [
        'Date', 'Verifier', 'University_CN', 'Country_CN', 
        'Job_CN', 'Direction', 'Extracted_Deadline'
    ]
    
    available_columns = [col for col in display_columns if col in filtered_data.columns]
    
    if available_columns:
        display_df = filtered_data[available_columns].copy()
        display_df['Date'] = display_df['Date'].dt.strftime('%Y-%m-%d')
        if 'Extracted_Deadline' in display_df.columns:
            display_df['Extracted_Deadline'] = pd.to_datetime(
                display_df['Extracted_Deadline'], errors='coerce'
            ).dt.strftime('%Y-%m-%d')
        
        st.dataframe(display_df, width='stretch', height=400)
        
        # ä¸‹è½½æŒ‰é’®
        csv = display_df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æ•°æ® (CSV)",
            data=csv,
            file_name=f"gisphere_performance_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )


# ==================== ä¸»ç¨‹åº ====================

def main():
    st.set_page_config(
        page_title="GISource ç»©æ•ˆé¢æ¿",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # æ ‡é¢˜
    st.title("ğŸ“Š GISource å›¢é˜Ÿç»©æ•ˆç®¡ç†é¢æ¿")
    st.markdown("---")
    
    # ä¾§è¾¹æ ï¼šæ§åˆ¶é¢æ¿
    with st.sidebar:
        st.header("âš™ï¸ ç­›é€‰æ¡ä»¶")
        
        # æ—¶é—´èŒƒå›´é€‰æ‹©
        days_options = {
            "æœ€è¿‘ 7 å¤©": 7,
            "æœ€è¿‘ 14 å¤©": 14,
            "æœ€è¿‘ 30 å¤©": 30,
            "æœ€è¿‘ 60 å¤©": 60,
            "æœ€è¿‘ 90 å¤©": 90,
            "æœ€è¿‘ 180 å¤©": 180,
            "æœ€è¿‘ 365 å¤©": 365,
            "å…¨éƒ¨æ•°æ®": 36500
        }
        
        selected_range = st.selectbox(
            "æ—¶é—´èŒƒå›´",
            options=list(days_options.keys()),
            index=2
        )
        days = days_options[selected_range]
        
        st.markdown("---")
        
        # åˆ·æ–°æŒ‰é’®
        if st.button("ğŸ”„ åˆ·æ–°æ•°æ®", width='stretch'):
            st.cache_resource.clear()
            st.rerun()
        
        st.markdown("---")
        st.info("ğŸ’¡ **æç¤º**: æ•°æ®æ¯æ¬¡åˆ·æ–°æ—¶ä¼šä» Google Sheet å’Œ MySQL æ•°æ®åº“é‡æ–°è¯»å–")
    
    # åŠ è½½å’Œåˆå¹¶æ•°æ®
    with st.spinner('æ­£åœ¨åŠ è½½æ•°æ®...'):
        data = merge_data()
    
    if data.empty:
        st.error("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œè¯·æ£€æŸ¥ Google Sheet å’Œæ•°æ®åº“è¿æ¥")
        return
    
    # æ ¹æ®æ—¶é—´èŒƒå›´ç­›é€‰æ•°æ®
    today = datetime.now(china_tz).date()
    start_date = today - timedelta(days=days)
    filtered_data = data[data['Date'].dt.date >= start_date]
    
    st.success(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®ï¼Œå½“å‰æ˜¾ç¤º {len(filtered_data)} æ¡æ•°æ®")
    
    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
    display_kpi_metrics(filtered_data)
    
    st.markdown("---")
    
    # å¸ƒå±€ï¼šä¸¤åˆ—
    col1, col2 = st.columns(2)
    
    with col1:
        display_member_leaderboard(filtered_data)
        st.markdown("---")
        display_country_distribution(filtered_data)
    
    with col2:
        display_lead_time_analysis(filtered_data)
        st.markdown("---")
        display_job_type_distribution(filtered_data)
    
    st.markdown("---")
    
    # æ¯æ—¥è¶‹åŠ¿ï¼ˆå…¨å®½ï¼‰
    display_daily_trend(filtered_data)
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: gray;'>"
        "GISource å›¢é˜Ÿç»©æ•ˆç®¡ç†ç³»ç»Ÿ | "
        f"æœ€åæ›´æ–°: {datetime.now(china_tz).strftime('%Y-%m-%d %H:%M:%S')}"
        "</div>",
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()

